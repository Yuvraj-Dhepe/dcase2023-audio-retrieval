{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the .pkl file\n",
    "file_path = \"../data/random_seed_experiment/seed_split_9_replication_3/audio_info.pkl\"  # Replace with the actual path to your file\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print the contents\n",
    "print(len(data['audio_fid2fname']['development']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def rename_folders_from_csv(folder_path, csv_file):\n",
    "    # Load the CSV data into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        original_name = row[\"Name\"]  # Folder name to look for\n",
    "        new_name = row[\"id\"]  # New name based on 'id' column\n",
    "\n",
    "        # Construct the full path of the original and new folder names\n",
    "        original_folder_path = os.path.join(folder_path, original_name)\n",
    "        new_folder_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "        # Check if the original folder exists\n",
    "        if os.path.exists(original_folder_path):\n",
    "            # Rename the folder\n",
    "            os.rename(original_folder_path, new_folder_path)\n",
    "            print(f'Renamed folder: \"{original_name}\" to \"{new_name}\"')\n",
    "        else:\n",
    "            print(\n",
    "                f'Folder \"{original_name}\" does not exist in the specified path.'\n",
    "            )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"z_ckpts\"  # Replace with your parent folder path\n",
    "csv_file = \"z_results/sweep_1_oct_29_2024_60_runs.csv\"  # Replace with your CSV file path\n",
    "\n",
    "rename_folders_from_csv(folder_path, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constants\n",
    "# times_the_original = 5\n",
    "# tids = 3839*(5+times_the_original)  # Number of TIDs #23034\n",
    "# fids_per_tid = 3839 *(1+ times_the_original)  # Number of FIDs per TID #7678\n",
    "# tuple_size = 81 + 24 + 24  # Size of each tuple (FID + score + is_relevant) in bytes\n",
    "\n",
    "\n",
    "# # Memory for one TID and its list of tuples\n",
    "# memory_per_tid = 81 + fids_per_tid * tuple_size  # 32 bytes for the TID itself\n",
    "\n",
    "\n",
    "# # Total memory for all TIDs\n",
    "# total_memory_in_bytes = tids * memory_per_tid\n",
    "\n",
    "\n",
    "# # Convert memory to megabytes (1 MB = 1024 * 1024 bytes)\n",
    "# total_memory_in_MB = total_memory_in_bytes / (1024 * 1024)\n",
    "# total_memory_in_MB # Comes out to be 21759 mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"47618d84c64aa733128b0ff7e395fbbe96304b6c\")\n",
    "# # Load configuration from conf.yaml\n",
    "# with open(\"conf_yamls/cap_0_conf.yaml\", \"rb\") as stream:\n",
    "#     conf = yaml.full_load(stream)\n",
    "\n",
    "# # Extract WandB configuration\n",
    "# wandb_conf = conf.get(\"wandb_conf\", {})\n",
    "# api = wandb.Api()\n",
    "# runs = api.runs(wandb_conf[\"project\"])  # Replace with your actual project name\n",
    "# # Get the latest run\n",
    "# latest_run = runs[len(runs) - 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Load CSV data\n",
    "csv_file = \"z_results/sweep_1_oct_29_2024_60_runs.csv\"  # Replace with your CSV filename\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Add a new column for IDs\n",
    "df[\"id\"] = None\n",
    "\n",
    "# Fetch the runs from wandb\n",
    "runs = api.runs(\"dcase2023-audio-retrieval\")\n",
    "\n",
    "# Update the DataFrame with the corresponding IDs\n",
    "for run in runs:\n",
    "    # Find matching rows in the DataFrame based on name\n",
    "    matching_rows = df[\"Name\"] == run.name\n",
    "    if matching_rows.any():\n",
    "        df.loc[matching_rows, \"id\"] = run.id\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(\"CSV file updated with run IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"dcase2023-audio-retrieval\", id=\"aj5hiz5c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"rough_log\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    \"conf_yamls/base_configs/cap_1x_conf.yaml\": [\"vj2b6kld\", 3.45993],\n",
    "    \"conf_yamls/base_configs/cap_2x_conf.yaml\": [\"73wk6hjv\", 3.49977],\n",
    "    \"conf_yamls/base_configs/cap_3x_conf.yaml\": [\"dkvnyj0u\", 3.42770],\n",
    "    \"conf_yamls/base_configs/cap_4x_conf.yaml\": [\"e0s30cv1\", 3.46872],\n",
    "    \"conf_yamls/base_configs/cap_5x_conf.yaml\": [\"xunedzbr\", 3.70214],\n",
    "    \"conf_yamls/base_configs/cap_0_new_conf.yaml\": [\"tqhxk9vc\", 3.40405],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "project_name = \"audio_dl_baseline\"\n",
    "api = wandb.Api()\n",
    "runs = api.runs(project_name)\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"rb\") as stream:\n",
    "        conf = yaml.full_load(stream)\n",
    "    return conf\n",
    "\n",
    "\n",
    "for key, value in pairs.items():\n",
    "    run_id = value[0]\n",
    "    val_obj = value[1]\n",
    "    print(val_obj)\n",
    "    if run_id:\n",
    "        conf = load_config(key)\n",
    "        latest_run = next(run for run in runs if run.id == run_id)\n",
    "        wandb.init(\n",
    "            project=project_name, id=run_id, resume=\"allow\", config=conf\n",
    "        )\n",
    "        wandb.log({\"val_obj_manual\": val_obj})\n",
    "        # latest_run.summary[\"val_obj_manual\"] = val_obj\n",
    "        # latest_run.summary.update()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\"dcase2023-audio-retrieval\")\n",
    "for j, i in enumerate(runs):\n",
    "    print(\"num =\", j, \"run name = \", i.name, \" id: \", i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(wandb_conf[\"project\"])\n",
    "for j, i in enumerate(runs):\n",
    "    print(\"num =\", j, \"run name = \", i.name, \" id: \", i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data for each dataset (train, val, eval) to build tid2fid\n",
    "for name in data_conf.keys():\n",
    "    params = data_conf[name]\n",
    "    name = name.replace(\"_data\", \"\")\n",
    "\n",
    "    # Load text data in chunks to minimize memory usage\n",
    "    text_fpath = os.path.join(params[\"dataset\"], params[\"text_data\"])\n",
    "    text_data = pd.read_csv(text_fpath, converters={\"tokens\": literal_eval})\n",
    "    print(\"\\n\\nLoaded\", text_fpath)\n",
    "\n",
    "    # Initialize tid2fid mapping\n",
    "    tid2fid = {}\n",
    "\n",
    "    # Process data chunk by chunk\n",
    "    for idx in text_data.index:\n",
    "        item = text_data.iloc[idx]\n",
    "        tid2fid[item[\"tid\"]] = item[\"fid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as WaveForm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Waveform is a visual representation of the audio signal, where the x-axis represents time and the y-axis represents amplitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig_to_pdf(fig, save_name):\n",
    "    \"\"\"\n",
    "    Saves the given plt figure object to a PDF.\n",
    "\n",
    "    :param fig: matplotlib.pyplot figure object to be saved.\n",
    "    :param save_name: The name of the file to save the figure as.\n",
    "    \"\"\"\n",
    "    # Save the figure as a PDF with the given file name\n",
    "    fig.savefig(save_name, dpi=150)\n",
    "    print(f\"Waveform saved as PDF at: {save_name}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_waveform(input_file):\n",
    "    \"\"\"\n",
    "    Plots the waveform for the given audio file.\n",
    "\n",
    "    :param input_file: Path to the input audio file.\n",
    "    :return: plt.figure object.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    array, sampling_rate = librosa.load(input_file)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(\n",
    "        figsize=(12, 4)\n",
    "    )  # Set the figure size (width x height in inches)\n",
    "    librosa.display.waveshow(\n",
    "        array, sr=sampling_rate, color=\"#800080\"\n",
    "    )  # Plot the waveform\n",
    "\n",
    "    # Beautify the plot\n",
    "    plt.title(\"Audio Waveform\", fontsize=16)\n",
    "    plt.xlabel(\"Time (seconds)\", fontsize=14)\n",
    "    plt.ylabel(\"Amplitude\", fontsize=14)\n",
    "    plt.grid(True, linestyle=\"-\", alpha=0.5)  # Add a grid\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "    return array,sampling_rate, fig\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_file = \"../example/chant.mp3\"\n",
    "array, sr, fig = plot_waveform(input_file)  # Generate the plot\n",
    "save_fig_to_pdf(fig, \"../example/01_waveform.pdf\")  # Save the plot as a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as Frequency Spectrum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency Spectrumrum plots the strength of the various frequency components that are present in this audio segment. The frequency values are on the x-axis, usually plotted on a logarithmic scale, while their amplitudes are on the y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_dft(input_file, num_samples=500000):\n",
    "    \"\"\"\n",
    "    Plots the DFT (Discrete Fourier Transform) of the given audio file.\n",
    "\n",
    "    :param input_file: Path to the input audio file.\n",
    "    :param num_samples: Number of samples to use from the beginning of the audio file for DFT.\n",
    "    :return: plt.figure object.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    array, sr = librosa.load(input_file)\n",
    "\n",
    "    # Taking only the first num_samples for better visualization\n",
    "    dft_input = array[:num_samples]\n",
    "\n",
    "    # Compute the DFT of the input signal\n",
    "    window = np.hanning(len(dft_input))\n",
    "    windowed_input = dft_input * window\n",
    "    dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "    # Get the amplitude spectrum in decibels\n",
    "    amplitude = np.abs(dft)\n",
    "    amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "    # Get the frequency bins\n",
    "    frequency = librosa.fft_frequencies(sr=sr, n_fft=len(dft_input))\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 4))  # Set the figure size (width x height in inches)\n",
    "    plt.plot(frequency, amplitude_db)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude (dB)\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.title(\"DFT of Audio Signal\", fontsize=16)\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_dft(input_file)\n",
    "save_fig_to_pdf(fig, \"../example/02_dft.pdf\")  # Save the plot as a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as Spectrogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spectrum only shows a frozen snapshot of the frequencies at a given instant.\n",
    "- The solution is to take multiple DFTs, each covering only a small slice of time, and stack the resulting spectra together into a spectrogram.\n",
    "- A spectrogram plots the frequency content of an audio signal as it changes over time. It allows you to see time, frequency, and amplitude all on one graph. The algorithm that performs this computation is the STFT or Short Time Fourier Transform.\n",
    "- The spectrogram is one of the most informative audio tools available to you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_stft(input_file):\n",
    "    \"\"\"\n",
    "    Plots the Short-Time Fourier Transform (STFT) of the given audio file.\n",
    "\n",
    "    :param input_file: Path to the input audio file.\n",
    "    :return: plt.figure object.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    array, sr = librosa.load(input_file)\n",
    "\n",
    "    # Compute the Short-Time Fourier Transform (STFT)\n",
    "    D = librosa.stft(array)\n",
    "\n",
    "    # Convert the amplitude to decibels\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 4))  # Set the figure size (width x height in inches)\n",
    "    librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.title(\"STFT Magnitude (in dB)\", fontsize=16)\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "input_file = librosa.ex(\"trumpet\")  # Example trumpet sound from librosa\n",
    "fig = plot_stft(input_file)  # Generate the STFT plot\n",
    "save_fig_to_pdf(fig, \"../example/03_stft.pdf\")  # Save the plot as a PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A mel spectrogram is a variation of the spectrogram that is commonly used in speech processing and machine learning tasks. It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but on a different frequency axis.\n",
    "- In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases. The mel scale is a perceptual scale that approximates the non-linear frequency response of the human ear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_melspectrogram(input_file, n_mels=128, fmax=8000):\n",
    "    \"\"\"\n",
    "    Plots the Mel spectrogram of the given audio file.\n",
    "\n",
    "    :param input_file: Path to the input audio file.\n",
    "    :param n_mels: Number of Mel bands (default: 128).\n",
    "    :param fmax: Maximum frequency for Mel scale (default: 8000 Hz).\n",
    "    :return: plt.figure object.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    array, sr = librosa.load(input_file)\n",
    "\n",
    "    # Compute the Mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=array, sr=sr, n_mels=n_mels, fmax=fmax)\n",
    "\n",
    "    # Convert the Mel spectrogram to decibels\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 4))  # Set the figure size (width x height in inches)\n",
    "    librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sr, fmax=fmax)\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.title(f\"Mel Spectrogram (n_mels={n_mels}, fmax={fmax} Hz)\", fontsize=16)\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "input_file = librosa.ex(\"trumpet\")  # Example trumpet sound from librosa\n",
    "fig = plot_melspectrogram(input_file)  # Generate the Mel spectrogram plot\n",
    "save_fig_to_pdf(fig, \"../example/04_melspectrogram.pdf\")  # S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_filter_bank(num_filters=10, num_fft_bins=128, mel_min=0, mel_max=1100):\n",
    "    \"\"\"\n",
    "    Generates a Mel filter bank and returns the figure object.\n",
    "\n",
    "    Parameters:\n",
    "        num_filters (int): Number of Mel filters.\n",
    "        num_fft_bins (int): Number of FFT bins.\n",
    "        mel_min (float): Minimum value in the Mel scale.\n",
    "        mel_max (float): Maximum value in the Mel scale.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object with the Mel filter bank plot.\n",
    "    \"\"\"\n",
    "    # Generate linearly spaced points in the Mel scale\n",
    "    mel_points = np.linspace(mel_min, mel_max, num_filters + 2)\n",
    "    bin_points = np.floor(mel_points / mel_max * (num_fft_bins - 1)).astype(int)\n",
    "\n",
    "    # Initialize filter bank\n",
    "    filter_bank = np.zeros((num_filters, num_fft_bins))\n",
    "\n",
    "    # Create triangular Mel filters\n",
    "    for i in range(1, len(bin_points) - 1):\n",
    "        start, center, end = bin_points[i - 1], bin_points[i], bin_points[i + 1]\n",
    "\n",
    "        # Left slope\n",
    "        filter_bank[i - 1, start:center + 1] = np.linspace(0, 1, center - start + 1)\n",
    "        # Right slope\n",
    "        filter_bank[i - 1, center:end + 1] = np.linspace(1, 0, end - center + 1)\n",
    "\n",
    "    # Plot the Mel filter bank\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i in range(num_filters):\n",
    "        ax.plot(filter_bank[i], label=f\"Filter {i + 1}\")\n",
    "    ax.set_title(\"Mel Filter Bank\")\n",
    "    ax.set_xlabel(\"FFT Bin Index\")\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    return fig\n",
    "\n",
    "# Generate the Mel filter bank figure\n",
    "mel_filter_fig = create_mel_filter_bank()\n",
    "save_fig_to_pdf(mel_filter_fig, \"../example/05_mel_filter_bank.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = minds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the int classes to strings\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the audio data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Audio Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    \"openai/whisper-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.map(prepare_dataset)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_sample_data(num_points):\n",
    "    \"\"\"Generates sample data with random relevance.\"\"\"\n",
    "    data = []\n",
    "    for i in range(num_points):\n",
    "        score = random.random()  # Random score between 0 and 1\n",
    "        is_relevant = random.randint(0, 1)  # Random relevance (0 or 1)\n",
    "        data.append((i + 1, score, is_relevant))  # Object ID is sequential\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_r1_r5(data):\n",
    "    \"\"\"Calculates R1 and R5 for a list of (object_id, score, is_relevant) tuples.\"\"\"\n",
    "    num_relevant = sum([item[2] for item in data])\n",
    "    if num_relevant == 0:\n",
    "        return 0, 0  # Handle cases with no relevant items\n",
    "    r1 = sum([item[2] for item in data[:1]]) / num_relevant\n",
    "    r5 = sum([item[2] for item in data[:5]]) / num_relevant\n",
    "    return r1, r5\n",
    "\n",
    "\n",
    "# Generate sample data\n",
    "num_points = 100\n",
    "data = generate_sample_data(num_points)\n",
    "\n",
    "# Sort data by score in descending order (important for R1/R5 calculation)\n",
    "data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Chunk the data\n",
    "chunk_size = 20\n",
    "chunks = [data[i : i + chunk_size] for i in range(0, num_points, chunk_size)]\n",
    "\n",
    "# Calculate metrics for each chunk\n",
    "chunk_metrics = [calculate_r1_r5(chunk) for chunk in chunks]\n",
    "\n",
    "# Calculate overall metrics for the entire dataset\n",
    "overall_metrics = calculate_r1_r5(data)\n",
    "\n",
    "# Aggregate chunk metrics\n",
    "avg_r1 = sum([m[0] for m in chunk_metrics]) / len(chunk_metrics)\n",
    "avg_r5 = sum([m[1] for m in chunk_metrics]) / len(chunk_metrics)\n",
    "\n",
    "# Print results\n",
    "print(\"Chunk Metrics (R1, R5):\")\n",
    "for i, metrics in enumerate(chunk_metrics):\n",
    "    print(f\"Chunk {i + 1}: {metrics}\")\n",
    "\n",
    "print(\"\\nOverall Metrics (R1, R5):\", overall_metrics)\n",
    "print(\"\\nAveraged Chunk Metrics (R1, R5):\", (avg_r1, avg_r5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
