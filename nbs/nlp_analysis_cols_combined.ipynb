{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvidh/micromamba/envs/audio_dl/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import openai\n",
    "from bertopic.representation import (\n",
    "    KeyBERTInspired,\n",
    "    MaximalMarginalRelevance,\n",
    "    OpenAI,\n",
    "    PartOfSpeech,\n",
    ")\n",
    "\n",
    "# Load your CSV data\n",
    "evaluation_folder = \"./curated_clotho_captions\"\n",
    "evaluation_csv = os.path.join(\n",
    "    evaluation_folder, \"clotho_captions_evaluation.csv\"\n",
    ")\n",
    "\n",
    "develpment_folder = \"./curated_clotho_captions\"\n",
    "development_csv = os.path.join(\n",
    "    develpment_folder, \"clotho_captions_development.csv\"\n",
    ")\n",
    "\n",
    "validation_folder = \"./curated_clotho_captions\"\n",
    "validation_csv = os.path.join(\n",
    "    validation_folder, \"clotho_captions_validation.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.read_csv(development_csv)\n",
    "\n",
    "\n",
    "# Extract captions columns\n",
    "caption_columns = [f\"caption_{i}\" for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              A muddled noise of broken channel of the TV\n",
       "1                 A person is turning a map over and over.\n",
       "2        Several barnyard animals mooing in a barn whil...\n",
       "3        An office chair is squeaking as someone bends ...\n",
       "4        A flying bee is buzzing loudly around an objec...\n",
       "                               ...                        \n",
       "19190             Metal chimes being chimed one at a time.\n",
       "19191          Going through all of the trash can noisily.\n",
       "19192           A match is lit, then another match is lit.\n",
       "19193       Sticks crunch and break while being walked on.\n",
       "19194           Crushing up food to make a dish for others\n",
       "Name: caption_text, Length: 19195, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_captions(df, caption_columns):\n",
    "    # Use pd.melt to transform the wide caption columns into a single concatenated column\n",
    "    melted_df = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"file_name\"],\n",
    "        value_vars=caption_columns,\n",
    "        var_name=\"caption_type\",\n",
    "        value_name=\"caption_text\",\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing captions if any\n",
    "    melted_df = melted_df.dropna(subset=[\"caption_text\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return melted_df[[\"file_name\", \"caption_text\"]]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "caption_columns = [\n",
    "    \"caption_1\",\n",
    "    \"caption_2\",\n",
    "    \"caption_3\",\n",
    "    \"caption_4\",\n",
    "    \"caption_5\",\n",
    "]\n",
    "\n",
    "# Assuming 'data' is the DataFrame you're working with\n",
    "concatenated_df = concatenate_captions(data, caption_columns)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "concatenated_df[\"caption_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" I have following topic labels and their keywords:\n",
    "# 1. **Human Activity**: talking, people, speaking, walking, running, chatting, conversing, footsteps, crowd, conversations\n",
    "\n",
    "# 2. **Machinery**: machine, machinery, whirring, factory, grinding, mechanical, motor, industrial\n",
    "\n",
    "# 3. **Bird and Insect Sounds**: chirping, chirps, chirp, birds, squawking, crickets, bird, crows, quacking, singing, buzzing, wind, crickets, chimes, whistling\n",
    "\n",
    "# 4. **Water & Water Flowing Sounds**: shower, waterfall, faucet, water, splashing, splashes, sink, flowing, fountain, dripping, pouring,drips\n",
    "\n",
    "# 5. **Vehicle Engines**: revving, car, cars, engine, engines, vehicle, vehicles, driving, motor, drive\n",
    "\n",
    "# 6. **Rain and Storm**: downpour, raining, rain, rainstorm, rainfall, thunderstorm, raindrops, torrential, thunder, hail, storm\n",
    "\n",
    "# 7. **Crowd Noise with Footsteps**: crowd, crowded, talk, talks, conversations, chatter, speaks, walking, walks, walk, walked, footsteps, stepping, hiking, shoes, boots, leaves, snow, underfoot\n",
    "\n",
    "# 9. **Door Sounds**: door, doors, hinges, opened, opens, creaking, opening, creaks, open, hinge, banging\n",
    "\n",
    "# 10. **Train and Railway Sounds**: train, trains, locomotive, railway, railroad, rail, tracks, whistle, track, squealing, sounds, station, passing, clacking, trolley, honking\n",
    "\n",
    "# 11. **Wind and Ocean Sounds**: windy, wind, waves, blowing, ocean, breeze, beach, shore, blows, storm, sea, gusts, whistling, gust\n",
    "\n",
    "# 12. **General Noises and Loud Sounds**: noises, noise, sounds, sound, loud, louder, tapping, metallic, blaring, sound, while, they\n",
    "\n",
    "\n",
    "Now I have a topic which contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "From the above 12 topic labels, assign a topic label that suits perfectly to the topic of the documents and it's keywords. Make sure it exactly is  of the format & Don't put any extra comments describing your reasoning:\n",
    "\n",
    "topic: <topic label>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load SentenceTransformer model for embeddings\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"dunzhang/stella_en_400M_v5\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# UMAP and HDBSCAN models as defined\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=18,\n",
    "    n_components=6,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=120,\n",
    "    min_samples=90,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "# CountVectorizer model as defined\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\", min_df=2, ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# OpenAI model definition (Make sure your OpenAI setup is correct)\n",
    "openai_client = openai.OpenAI(\n",
    "    base_url=\"http://172.18.176.1:11434/v1\",\n",
    "    api_key=\"ollama\",  # required, but unused\n",
    ")\n",
    "# prompt = \"\"\"\n",
    "# I have a topic that contains the following documents:\n",
    "# [DOCUMENTS]\n",
    "# The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "# Based on the information above, extract a highly descriptive topic label that very well defines the topic. Make sure it is in the following format:\n",
    "# topic: <topic label>, and it should not have any Topic word in it.\n",
    "# \"\"\"\n",
    "openai_model = OpenAI(\n",
    "    client=openai_client,\n",
    "    model=\"llama3.1\",\n",
    "    exponential_backoff=True,\n",
    "    chat=True,\n",
    "    prompt=prompt,\n",
    "    diversity=1,\n",
    "    nr_docs=9,\n",
    ")\n",
    "\n",
    "# Representation models\n",
    "representation_models = {\n",
    "    \"KeyBERT\": KeyBERTInspired(),\n",
    "    \"OpenAI\": openai_model,\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.6),\n",
    "    \"POS\": PartOfSpeech(\"en_core_web_sm\"),\n",
    "}\n",
    "\n",
    "# Initialize BERTopic model without fitting yet\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_models,\n",
    "    top_n_words=10,\n",
    "    verbose=True,\n",
    "    min_topic_size=15,\n",
    "    nr_topics=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 600/600 [00:19<00:00, 30.76it/s]\n",
      "2024-09-09 13:44:40,240 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "/home/yuvidh/micromamba/envs/audio_dl/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "2024-09-09 13:44:57,750 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-09-09 13:44:57,751 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-09-09 13:44:58,616 - BERTopic - Cluster - Completed ✓\n",
      "2024-09-09 13:44:58,617 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 27/27 [00:15<00:00,  1.77it/s]\n",
      "2024-09-09 13:45:17,984 - BERTopic - Representation - Completed ✓\n",
      "2024-09-09 13:45:17,985 - BERTopic - Topic reduction - Reducing number of topics\n",
      "100%|██████████| 13/13 [00:06<00:00,  1.90it/s]\n",
      "2024-09-09 13:45:26,983 - BERTopic - Topic reduction - Reduced number of topics from 27 to 13\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "caption_column = \"all_caps\"\n",
    "# Extract the captions from the current column\n",
    "captions = concatenated_df[\"caption_text\"].tolist()\n",
    "\n",
    "# Generate embeddings for captions\n",
    "embeddings = embedding_model.encode(captions, show_progress_bar=True)\n",
    "\n",
    "# Fit the BERTopic model to the current column's captions\n",
    "topics, probs = topic_model.fit_transform(captions, embeddings)\n",
    "# topic_model.reduce_topics(captions, nr_topics=10)\n",
    "# Get the topic labels\n",
    "topic_labels = topic_model.get_topic_info()\n",
    "topic_labels_dict = topic_labels.set_index(\"Topic\")[\"Name\"].to_dict()\n",
    "\n",
    "# Handle -1 topics (documents not assigned to any topic)\n",
    "topic_labels_dict[-1] = \"IRRelevant\"\n",
    "\n",
    "# Add topic labels and representations to the original DataFrame\n",
    "concatenated_df[f\"{caption_column}_topic\"] = [\n",
    "    topic_labels_dict.get(topic, \"No topic assigned\") for topic in topics\n",
    "]\n",
    "concatenated_df[f\"{caption_column}_KeyBERT\"] = [\n",
    "    (\n",
    "        topic_labels.loc[topic_labels[\"Topic\"] == topic, \"KeyBERT\"].values[0]\n",
    "        if topic in topic_labels_dict\n",
    "        else \"No topic assigned\"\n",
    "    )\n",
    "    for topic in topics\n",
    "]\n",
    "concatenated_df[f\"{caption_column}_OpenAI\"] = [\n",
    "    (\n",
    "        topic_labels.loc[topic_labels[\"Topic\"] == topic, \"OpenAI\"].values[0]\n",
    "        if topic in topic_labels_dict\n",
    "        else \"No topic assigned\"\n",
    "    )\n",
    "    for topic in topics\n",
    "]\n",
    "concatenated_df[f\"{caption_column}_MMR\"] = [\n",
    "    (\n",
    "        topic_labels.loc[topic_labels[\"Topic\"] == topic, \"MMR\"].values[0]\n",
    "        if topic in topic_labels_dict\n",
    "        else \"No topic assigned\"\n",
    "    )\n",
    "    for topic in topics\n",
    "]\n",
    "concatenated_df[f\"{caption_column}_POS\"] = [\n",
    "    (\n",
    "        topic_labels.loc[topic_labels[\"Topic\"] == topic, \"POS\"].values[0]\n",
    "        if topic in topic_labels_dict\n",
    "        else \"No topic assigned\"\n",
    "    )\n",
    "    for topic in topics\n",
    "]\n",
    "\n",
    "# Count the number of occurrences of each topic\n",
    "topic_counts = Counter(topics)\n",
    "\n",
    "# Store the topic counts for the current caption column\n",
    "all_topic_counts = {\n",
    "    topic: {\n",
    "        \"Count\": count,\n",
    "        \"KeyBERT\": (\n",
    "            topic_labels.loc[topic_labels[\"Topic\"] == topic, \"KeyBERT\"].values[\n",
    "                0\n",
    "            ]\n",
    "            if topic in topic_labels_dict\n",
    "            else \"No topic assigned\"\n",
    "        ),\n",
    "        \"OpenAI\": (\n",
    "            topic_labels.loc[topic_labels[\"Topic\"] == topic, \"OpenAI\"].values[\n",
    "                0\n",
    "            ]\n",
    "            if topic in topic_labels_dict\n",
    "            else \"No topic assigned\"\n",
    "        ),\n",
    "        \"MMR\": (\n",
    "            topic_labels.loc[topic_labels[\"Topic\"] == topic, \"MMR\"].values[0]\n",
    "            if topic in topic_labels_dict\n",
    "            else \"No topic assigned\"\n",
    "        ),\n",
    "        \"POS\": (\n",
    "            topic_labels.loc[topic_labels[\"Topic\"] == topic, \"POS\"].values[0]\n",
    "            if topic in topic_labels_dict\n",
    "            else \"No topic assigned\"\n",
    "        ),\n",
    "    }\n",
    "    for topic, count in topic_counts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "def create_topic_df(topic_dict):\n",
    "    # Initialize an empty list to hold the rows\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through the dictionary to create rows for the DataFrame\n",
    "    for topic_num, topic_info in topic_dict.items():\n",
    "        # Create a row for each topic\n",
    "        row = {\n",
    "            \"Topic\": topic_num,\n",
    "            \"Count\": topic_info[\"Count\"],\n",
    "            \"KeyBERT\": \", \".join(topic_info[\"KeyBERT\"]),\n",
    "            \"OpenAI\": \", \".join(topic_info[\"OpenAI\"]),\n",
    "            \"MMR\": \", \".join(topic_info[\"MMR\"]),\n",
    "            \"POS\": \", \".join(topic_info[\"POS\"]),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create the DataFrame\n",
    "topic_df = create_topic_df(all_topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.to_csv(\"topics_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the all_topic_counts dictionary\n",
    "topic_counts_df = pd.DataFrame(\n",
    "    {\n",
    "        column: pd.Series(topic_counts)\n",
    "        for column, topic_counts in all_topic_counts.items()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Reset the index to create a multi-index DataFrame\n",
    "topic_counts_df = topic_counts_df.stack().reset_index()\n",
    "topic_counts_df.columns = [\"Topic\", \"Caption Column\", \"Topic Info\"]\n",
    "\n",
    "# # Convert the 'Topic Info' column to a dictionary\n",
    "# topic_counts_df['Topic Info'] = topic_counts_df['Topic Info'].apply(lambda x: dict(x))\n",
    "\n",
    "# # Expand the 'Topic Info' dictionary into separate columns\n",
    "# topic_counts_df = pd.concat([topic_counts_df.drop('Topic Info', axis=1), topic_counts_df['Topic Info'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# # Rename the columns\n",
    "# topic_counts_df.columns = ['Topic', 'Caption Column', 'Count', 'KeyBERT', 'OpenAI', 'MMR', 'POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topic_counts_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./curated_clotho_captions/topics_development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your existing DataFrame\n",
    "# Split the DataFrame into separate DataFrames for each unique 'Caption Column'\n",
    "unique_captions = df[\"Caption Column\"].unique()\n",
    "\n",
    "# Create a dictionary to store the separate DataFrames\n",
    "caption_dfs = {}\n",
    "\n",
    "for caption in unique_captions:\n",
    "    caption_dfs[caption] = df[df[\"Caption Column\"] == caption].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "# Now, caption_dfs will contain separate DataFrames for each 'Caption Column'\n",
    "# For example, you can access the DataFrame for 'caption_1' using:\n",
    "df_caption_1 = caption_dfs[\"caption_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def create_combined_wordcloud(df, ignore_topic=\"-1\", representation=\"KeyBERT\"):\n",
    "    # Validate ignore_topic and representation\n",
    "    if ignore_topic not in df[\"Topic\"].unique():\n",
    "        raise ValueError(\n",
    "            f\"Invalid ignore_topic '{ignore_topic}'. It should be one of {df['Topic'].unique()}\"\n",
    "        )\n",
    "    if representation not in [\"KeyBERT\", \"OpenAI\", \"MMR\", \"POS\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid representation '{representation}'. It should be one of ['KeyBERT', 'OpenAI', 'MMR', 'POS']\"\n",
    "        )\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    col_mapping = {\n",
    "        \"KeyBERT\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "        \"OpenAI\": lambda x: [word for word in x],\n",
    "        \"MMR\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "        \"POS\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_filtered = df[df[\"Topic\"] != ignore_topic]\n",
    "\n",
    "    # Apply the mapping function to the specified column\n",
    "    all_words = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        words = col_mapping[representation](row[representation])\n",
    "        all_words.extend(\n",
    "            words * row[\"Count\"]\n",
    "        )  # Use Count as weight for the words\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freqs = Counter(all_words)\n",
    "\n",
    "    # Generate and display the word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=500,\n",
    "        max_font_size=110,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "    ).generate_from_frequencies(word_freqs)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"{representation} Representation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_wordcloud(\n",
    "    caption_dfs[\"caption_1\"], ignore_topic=-1, representation=\"OpenAI\"\n",
    ")\n",
    "create_combined_wordcloud(\n",
    "    caption_dfs[\"caption_2\"], ignore_topic=-1, representation=\"OpenAI\"\n",
    ")\n",
    "create_combined_wordcloud(\n",
    "    caption_dfs[\"caption_3\"], ignore_topic=-1, representation=\"OpenAI\"\n",
    ")\n",
    "create_combined_wordcloud(\n",
    "    caption_dfs[\"caption_4\"], ignore_topic=-1, representation=\"OpenAI\"\n",
    ")\n",
    "create_combined_wordcloud(\n",
    "    caption_dfs[\"caption_5\"], ignore_topic=-1, representation=\"OpenAI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def create_combined_wordcloud(df, ignore_topic=\"-1\", representation=\"KeyBERT\"):\n",
    "    # Validate ignore_topic and representation\n",
    "    if ignore_topic not in df[\"Topic\"].unique():\n",
    "        raise ValueError(\n",
    "            f\"Invalid ignore_topic '{ignore_topic}'. It should be one of {df['Topic'].unique()}\"\n",
    "        )\n",
    "    if representation not in [\"KeyBERT\", \"OpenAI\", \"MMR\", \"POS\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid representation '{representation}'. It should be one of ['KeyBERT', 'OpenAI', 'MMR', 'POS']\"\n",
    "        )\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    col_mapping = {\n",
    "        \"KeyBERT\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "        \"OpenAI\": lambda x: [word for word in x],\n",
    "        \"MMR\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "        \"POS\": lambda x: [\n",
    "            word\n",
    "            for sentence in x\n",
    "            for word, _ in pos_tag(word_tokenize(sentence))\n",
    "            if word.lower() not in stop_words\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_filtered = df[df[\"Topic\"] != ignore_topic]\n",
    "\n",
    "    # Apply the mapping function to the specified column\n",
    "    all_words = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        words = col_mapping[representation](row[representation])\n",
    "        all_words.extend(\n",
    "            words * row[\"Count\"]\n",
    "        )  # Use Count as weight for the words\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freqs = Counter(all_words)\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=500,\n",
    "        max_font_size=110,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "    ).generate_from_frequencies(word_freqs)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "    # Plot the word cloud\n",
    "    ax[0].imshow(wc, interpolation=\"bilinear\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(f\"{representation} Representation\")\n",
    "\n",
    "    # Create color bar\n",
    "    color_map = plt.get_cmap(\"viridis\")\n",
    "    norm = mpl.colors.Normalize(\n",
    "        vmin=min(word_freqs.values()), vmax=max(word_freqs.values())\n",
    "    )\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=norm)\n",
    "    sm.set_array([])  # Only needed for older versions of Matplotlib\n",
    "\n",
    "    # Plot the color bar\n",
    "    cbar = fig.colorbar(\n",
    "        sm, ax=ax[1], orientation=\"vertical\", fraction=0.02, pad=0.04\n",
    "    )\n",
    "    ax[1].set_title(\"Word Frequency Color Bar\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Common Sounds\n",
    "# Sure, here are the unique topics derived from the captions:\n",
    "\n",
    "# 1. **Human Activity**: talking, people, speaking, walking, running, chatting, conversing, footsteps, crowd, conversations\n",
    "\n",
    "# 2. **Machinery**: machine, machinery, whirring, factory, grinding, mechanical, motor, industrial\n",
    "\n",
    "# 3. **Bird and Insect Sounds**: chirping, chirps, chirp, birds, squawking, crickets, bird, crows, quacking, singing, buzzing, wind, crickets, chimes, whistling\n",
    "\n",
    "# 4. **Water & Water Flowing Sounds**: shower, waterfall, faucet, water, splashing, splashes, sink, flowing, fountain, dripping, pouring,drips\n",
    "\n",
    "# 5. **Vehicle Engines**: revving, car, cars, engine, engines, vehicle, vehicles, driving, motor, drive\n",
    "\n",
    "# 6. **Rain and Storm**: downpour, raining, rain, rainstorm, rainfall, thunderstorm, raindrops, torrential, thunder, hail, storm\n",
    "\n",
    "# 7. **Crowd Noise with Footsteps**: crowd, crowded, talk, talks, conversations, chatter, speaks, walking, walks, walk, walked, footsteps, stepping, hiking, shoes, boots, leaves, snow, underfoot\n",
    "\n",
    "# 9. **Door Sounds**: door, doors, hinges, opened, opens, creaking, opening, creaks, open, hinge, banging\n",
    "\n",
    "# 10. **Train and Railway Sounds**: train, trains, locomotive, railway, railroad, rail, tracks, whistle, track, squealing, sounds, station, passing, clacking, trolley, honking\n",
    "\n",
    "# 11. **Wind and Ocean Sounds**: windy, wind, waves, blowing, ocean, breeze, beach, shore, blows, storm, sea, gusts, whistling, gust\n",
    "\n",
    "# 12. **General Noises and Loud Sounds**: noises, noise, sounds, sound, loud, louder, tapping, metallic, blaring, sound, while, they"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
