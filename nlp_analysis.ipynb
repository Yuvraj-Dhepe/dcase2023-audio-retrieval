{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BertTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import openai\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(\"data/Clotho/clotho_captions_development.csv\")\n",
    "\n",
    "# Extract captions columns\n",
    "caption_columns = [f\"caption_{i}\" for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentenceTransformer model for embeddings\n",
    "embedding_model = SentenceTransformer(\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)\n",
    "\n",
    "# UMAP and HDBSCAN models as defined\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# CountVectorizer model as defined\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# OpenAI model definition (Make sure your OpenAI setup is correct)\n",
    "openai_client = openai.OpenAI(\n",
    "    base_url='http://172.18.176.1:11434/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "openai_model = OpenAI(client=openai_client, model=\"llama3.1\", exponential_backoff=True, chat=True, prompt=prompt, diversity=1, nr_docs=9)\n",
    "\n",
    "# Representation models\n",
    "representation_models = {\n",
    "    \"KeyBERT\": KeyBERTInspired(),\n",
    "    \"OpenAI\": openai_model,\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.6),\n",
    "    \"POS\": PartOfSpeech(\"en_core_web_sm\")\n",
    "}\n",
    "\n",
    "# Initialize BERTopic model without fitting yet\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    # vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_models,\n",
    "    top_n_words=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Variable to store topic counts for each caption column\n",
    "all_topic_counts = {}\n",
    "\n",
    "# Iterate through each caption column and fit the BERTopic model\n",
    "for caption_column in caption_columns:\n",
    "    # Extract the captions from the current column\n",
    "    captions = data[caption_column].tolist()\n",
    "\n",
    "    # Generate embeddings for captions\n",
    "    embeddings = embedding_model.encode(captions, show_progress_bar=True)\n",
    "\n",
    "    # Fit the BERTopic model to the current column's captions\n",
    "    topics, probs = topic_model.fit_transform(captions, embeddings)\n",
    "\n",
    "    # Get the topic labels\n",
    "    topic_labels = topic_model.get_topic_info()\n",
    "    topic_labels_dict = topic_labels.set_index('Topic')['Name'].to_dict()\n",
    "\n",
    "    # Handle -1 topics (documents not assigned to any topic)\n",
    "    topic_labels_dict[-1] = \"IRRelevant\"\n",
    "\n",
    "    # Add topic labels and representations to the original DataFrame\n",
    "    data[f'{caption_column}_topic'] = [topic_labels_dict.get(topic, \"No topic assigned\") for topic in topics]\n",
    "    data[f'{caption_column}_KeyBERT'] = [topic_labels.loc[topic_labels['Topic'] == topic, 'KeyBERT'].values[0] if topic in topic_labels_dict else \"No topic assigned\" for topic in topics]\n",
    "    data[f'{caption_column}_OpenAI'] = [topic_labels.loc[topic_labels['Topic'] == topic, 'OpenAI'].values[0] if topic in topic_labels_dict else \"No topic assigned\" for topic in topics]\n",
    "    data[f'{caption_column}_MMR'] = [topic_labels.loc[topic_labels['Topic'] == topic, 'MMR'].values[0] if topic in topic_labels_dict else \"No topic assigned\" for topic in topics]\n",
    "    data[f'{caption_column}_POS'] = [topic_labels.loc[topic_labels['Topic'] == topic, 'POS'].values[0] if topic in topic_labels_dict else \"No topic assigned\" for topic in topics]\n",
    "\n",
    "    # Count the number of occurrences of each topic\n",
    "    topic_counts = Counter(topics)\n",
    "\n",
    "    # Store the topic counts for the current caption column\n",
    "    all_topic_counts[caption_column] = {topic: {\"Count\": count,\n",
    "                                                \"KeyBERT\": topic_labels.loc[topic_labels['Topic'] == topic, 'KeyBERT'].values[0] if topic in topic_labels_dict else \"No topic assigned\",\n",
    "                                                \"OpenAI\": topic_labels.loc[topic_labels['Topic'] == topic, 'OpenAI'].values[0] if topic in topic_labels_dict else \"No topic assigned\",\n",
    "                                                \"MMR\": topic_labels.loc[topic_labels['Topic'] == topic, 'MMR'].values[0] if topic in topic_labels_dict else \"No topic assigned\",\n",
    "                                                \"POS\": topic_labels.loc[topic_labels['Topic'] == topic, 'POS'].values[0] if topic in topic_labels_dict else \"No topic assigned\"}\n",
    "                                        for topic, count in topic_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_barchart(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the all_topic_counts dictionary\n",
    "topic_counts_df = pd.DataFrame({column: pd.Series(topic_counts) for column, topic_counts in all_topic_counts.items()})\n",
    "\n",
    "# Reset the index to create a multi-index DataFrame\n",
    "topic_counts_df = topic_counts_df.stack().reset_index()\n",
    "topic_counts_df.columns = ['Topic', 'Caption Column', 'Topic Info']\n",
    "\n",
    "# Convert the 'Topic Info' column to a dictionary\n",
    "topic_counts_df['Topic Info'] = topic_counts_df['Topic Info'].apply(lambda x: dict(x))\n",
    "\n",
    "# Expand the 'Topic Info' dictionary into separate columns\n",
    "topic_counts_df = pd.concat([topic_counts_df.drop('Topic Info', axis=1), topic_counts_df['Topic Info'].apply(pd.Series)], axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "topic_counts_df.columns = ['Topic', 'Caption Column', 'Count', 'KeyBERT', 'OpenAI', 'MMR', 'POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topic_counts_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your existing DataFrame\n",
    "# Split the DataFrame into separate DataFrames for each unique 'Caption Column'\n",
    "unique_captions = df['Caption Column'].unique()\n",
    "\n",
    "# Create a dictionary to store the separate DataFrames\n",
    "caption_dfs = {}\n",
    "\n",
    "for caption in unique_captions:\n",
    "    caption_dfs[caption] = df[df['Caption Column'] == caption].reset_index(drop = True)\n",
    "\n",
    "# Now, caption_dfs will contain separate DataFrames for each 'Caption Column'\n",
    "# For example, you can access the DataFrame for 'caption_1' using:\n",
    "df_caption_1 = caption_dfs['caption_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def create_combined_wordcloud(df, ignore_topic='-1', representation='KeyBERT'):\n",
    "    # Validate ignore_topic and representation\n",
    "    if ignore_topic not in df['Topic'].unique():\n",
    "        raise ValueError(f\"Invalid ignore_topic '{ignore_topic}'. It should be one of {df['Topic'].unique()}\")\n",
    "    if representation not in ['KeyBERT', 'OpenAI', 'MMR', 'POS']:\n",
    "        raise ValueError(f\"Invalid representation '{representation}'. It should be one of ['KeyBERT', 'OpenAI', 'MMR', 'POS']\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    col_mapping = {\n",
    "        'KeyBERT': lambda x:  [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words],\n",
    "        'OpenAI': lambda x: [word for word in x],\n",
    "        'MMR': lambda x: [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words],\n",
    "        'POS': lambda x: [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words]\n",
    "    }\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_filtered = df[df['Topic'] != ignore_topic]\n",
    "\n",
    "    # Apply the mapping function to the specified column\n",
    "    all_words = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        words = col_mapping[representation](row[representation])\n",
    "        all_words.extend(words * row['Count'])  # Use Count as weight for the words\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freqs = Counter(all_words)\n",
    "\n",
    "    # Generate and display the word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    wc = WordCloud(width=800, height=500, max_font_size=110, background_color='white', colormap='viridis').generate_from_frequencies(word_freqs)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{representation} Representation\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_wordcloud(caption_dfs['caption_1'], ignore_topic=-1, representation='OpenAI')\n",
    "create_combined_wordcloud(caption_dfs['caption_2'], ignore_topic=-1, representation='OpenAI')\n",
    "create_combined_wordcloud(caption_dfs['caption_3'], ignore_topic=-1, representation='OpenAI')\n",
    "create_combined_wordcloud(caption_dfs['caption_4'], ignore_topic=-1, representation='OpenAI')\n",
    "create_combined_wordcloud(caption_dfs['caption_5'], ignore_topic=-1, representation='OpenAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def create_combined_wordcloud(df, ignore_topic='-1', representation='KeyBERT'):\n",
    "    # Validate ignore_topic and representation\n",
    "    if ignore_topic not in df['Topic'].unique():\n",
    "        raise ValueError(f\"Invalid ignore_topic '{ignore_topic}'. It should be one of {df['Topic'].unique()}\")\n",
    "    if representation not in ['KeyBERT', 'OpenAI', 'MMR', 'POS']:\n",
    "        raise ValueError(f\"Invalid representation '{representation}'. It should be one of ['KeyBERT', 'OpenAI', 'MMR', 'POS']\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    col_mapping = {\n",
    "        'KeyBERT': lambda x:  [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words],\n",
    "        'OpenAI': lambda x: [word for word in x],\n",
    "        'MMR': lambda x: [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words],\n",
    "        'POS': lambda x: [word for sentence in x for word, _ in pos_tag(word_tokenize(sentence)) if word.lower() not in stop_words]\n",
    "    }\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df_filtered = df[df['Topic'] != ignore_topic]\n",
    "\n",
    "    # Apply the mapping function to the specified column\n",
    "    all_words = []\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        words = col_mapping[representation](row[representation])\n",
    "        all_words.extend(words * row['Count'])  # Use Count as weight for the words\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freqs = Counter(all_words)\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wc = WordCloud(width=800, height=500, max_font_size=110, background_color='white', colormap='viridis').generate_from_frequencies(word_freqs)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "    # Plot the word cloud\n",
    "    ax[0].imshow(wc, interpolation='bilinear')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title(f\"{representation} Representation\")\n",
    "\n",
    "    # Create color bar\n",
    "    color_map = plt.get_cmap('viridis')\n",
    "    norm = mpl.colors.Normalize(vmin=min(word_freqs.values()), vmax=max(word_freqs.values()))\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=norm)\n",
    "    sm.set_array([])  # Only needed for older versions of Matplotlib\n",
    "\n",
    "    # Plot the color bar\n",
    "    cbar = fig.colorbar(sm, ax=ax[1], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    ax[1].set_title('Word Frequency Color Bar')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Common Sounds\n",
    "# Sure, here are the unique topics derived from the captions:\n",
    "\n",
    "# 1. **Human Activity**: talking, people, speaking, walking, running, chatting, conversing, footsteps, crowd, conversations\n",
    "# 2. **Machinery**: machine, machinery, whirring, factory, grinding, mechanical, motor, industrial\n",
    "# 3. **Bird and Insect Sounds**: chirping, chirps, chirp, birds, squawking, crickets, bird, crows, quacking, singing, buzzing\n",
    "# 4. **Water Sounds**: shower, waterfall, faucet, water, splashing, splashes, sink, flowing, fountain, dripping, pouring\n",
    "# 5. **Vehicle Engines**: revving, car, cars, engine, engines, vehicle, vehicles, driving, motor, drive\n",
    "# 6. **Rain and Storm**: downpour, raining, rain, rainstorm, rainfall, thunderstorm, raindrops, torrential, thunder, hail, storm\n",
    "# 7. **Crowd Noise**: crowd, crowded, talk, talks, conversations, chatter, speaks\n",
    "# 8. **Footsteps**: walking, walks, walk, walked, footsteps, stepping, hiking, shoes, boots, leaves, snow, underfoot\n",
    "# 9. **Door Sounds**: door, doors, hinges, opened, opens, creaking, opening, creaks, open, hinge, banging\n",
    "# 10. **Train and Railway Sounds**: train, trains, locomotive, railway, railroad, rail, tracks, whistle, track, squealing, sounds, station, passing, clacking, trolley, honking\n",
    "# 11. **Wind and Ocean Sounds**: windy, wind, waves, blowing, ocean, breeze, beach, shore, blows, storm, sea, gusts, whistling, gust\n",
    "# 12. **General Noises and Loud Sounds**: noises, noise, sounds, sound, loud, louder, tapping, metallic, blaring, sound, while, they\n",
    "# 13. **Water Flowing Sounds**: water, faucet, waterfall, splashing, pouring, sink, fountain, flowing, splashes, dripping, drips\n",
    "# 14. **Bird and Wind Sounds**: chirping, chirps, birds, chirp, whistling, bird, squawking, wind, crickets, chimes\n",
    "\n",
    "# This list combines the distinct topics from all provided captions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
