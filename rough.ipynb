{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuvi-kiit\u001b[0m (\u001b[33myuvi-kiit-university-of-potsdam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/yuvidh/.netrc\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import wandb\n",
    "# Load configuration from conf.yaml\n",
    "with open(\"conf_yamls/cap_0_conf.yaml\", \"rb\") as stream:\n",
    "    conf = yaml.full_load(stream)\n",
    "wandb.login(key = '47618d84c64aa733128b0ff7e395fbbe96304b6c')\n",
    "# Extract WandB configuration\n",
    "wandb_conf = conf.get(\"wandb_conf\", {})\n",
    "api = wandb.Api()\n",
    "runs = api.runs(wandb_conf['project'])  # Replace with your actual project name\n",
    "# Get the latest run\n",
    "latest_run = runs[len(runs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tqhxk9vc'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(wandb_conf['project'])\n",
    "for j,i in enumerate(runs):\n",
    "  print(\"num =\",j,\"run name = \",i.name,\" id: \", i.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import sys\n",
    "\n",
    "# Directory where your .db files are stored\n",
    "db_path = \"z_ckpts/nk56srrr/train_tid_2_items.db\"  # Replace with your actual directory path\n",
    "\n",
    "total_size = 0  # Variable to accumulate total memory usage\n",
    "\n",
    "with shelve.open(db_path) as db:\n",
    "    count = 0\n",
    "    for fid, group_scores in db.items():\n",
    "        print(fid, group_scores)\n",
    "        print(len(group_scores)) # 19195\n",
    "        break\n",
    "\n",
    "# Print the total memory usage\n",
    "print(f\"Total size of the entire database in RAM: {total_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as WaveForm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Waveform is a visual representation of the audio signal, where the x-axis represents time and the y-axis represents amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array, sampling_rate = librosa.load(\"data/Clotho/development/voice.wav\")\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as Frequency Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency Spectrumrum plots the strength of the various frequency components that are present in this audio segment. The frequency values are on the x-axis, usually plotted on a logarithmic scale, while their amplitudes are on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_input = array[:500] # taking only first 500 samples for better visualization\n",
    "\n",
    "# Compute the DFT of the input signal\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# Get the amplitude spectrum in decibals\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio as Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spectrum only shows a frozen snapshot of the frequencies at a given instant. \n",
    "- The solution is to take multiple DFTs, each covering only a small slice of time, and stack the resulting spectra together into a spectrogram.\n",
    "- A spectrogram plots the frequency content of an audio signal as it changes over time. It allows you to see time, frequency, and amplitude all on one graph. The algorithm that performs this computation is the STFT or Short Time Fourier Transform.\n",
    "- The spectrogram is one of the most informative audio tools available to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = librosa.stft(array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A mel spectrogram is a variation of the spectrogram that is commonly used in speech processing and machine learning tasks. It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but on a different frequency axis.\n",
    "- In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases. The mel scale is a perceptual scale that approximates the non-linear frequency response of the human ear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = load_dataset(\"PolyAI/minds14\", name = \"en-AU\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example= minds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the int classes to strings\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the audio data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.map(prepare_dataset)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_sample_data(num_points):\n",
    "    \"\"\"Generates sample data with random relevance.\"\"\"\n",
    "    data = []\n",
    "    for i in range(num_points):\n",
    "        score = random.random()  # Random score between 0 and 1\n",
    "        is_relevant = random.randint(0, 1)  # Random relevance (0 or 1)\n",
    "        data.append((i + 1, score, is_relevant))  # Object ID is sequential\n",
    "    return data\n",
    "\n",
    "def calculate_r1_r5(data):\n",
    "    \"\"\"Calculates R1 and R5 for a list of (object_id, score, is_relevant) tuples.\"\"\"\n",
    "    num_relevant = sum([item[2] for item in data])\n",
    "    if num_relevant == 0:\n",
    "        return 0, 0  # Handle cases with no relevant items\n",
    "    r1 = sum([item[2] for item in data[:1]]) / num_relevant\n",
    "    r5 = sum([item[2] for item in data[:5]]) / num_relevant\n",
    "    return r1, r5\n",
    "\n",
    "# Generate sample data\n",
    "num_points = 100\n",
    "data = generate_sample_data(num_points)\n",
    "\n",
    "# Sort data by score in descending order (important for R1/R5 calculation)\n",
    "data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Chunk the data\n",
    "chunk_size = 20\n",
    "chunks = [data[i:i + chunk_size] for i in range(0, num_points, chunk_size)]\n",
    "\n",
    "# Calculate metrics for each chunk\n",
    "chunk_metrics = [calculate_r1_r5(chunk) for chunk in chunks]\n",
    "\n",
    "# Calculate overall metrics for the entire dataset\n",
    "overall_metrics = calculate_r1_r5(data)\n",
    "\n",
    "# Aggregate chunk metrics\n",
    "avg_r1 = sum([m[0] for m in chunk_metrics]) / len(chunk_metrics)\n",
    "avg_r5 = sum([m[1] for m in chunk_metrics]) / len(chunk_metrics)\n",
    "\n",
    "# Print results\n",
    "print(\"Chunk Metrics (R1, R5):\")\n",
    "for i, metrics in enumerate(chunk_metrics):\n",
    "    print(f\"Chunk {i + 1}: {metrics}\")\n",
    "\n",
    "print(\"\\nOverall Metrics (R1, R5):\", overall_metrics)\n",
    "print(\"\\nAveraged Chunk Metrics (R1, R5):\", (avg_r1, avg_r5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
