# Configure training, validation, and evaluation data
# NOTE: text_data is filled with tid, fid, fname (audio_fname), raw_text (raw_caption), text (smallcase_without_any_punc_marks), tokens (for text field)
data_conf:
    train_data: # training data
        dataset: ./data/exp_5
        audio_data: development_audio_logmels.hdf5
        text_data: development_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence

    val_data: # validation data
        dataset: ./data/exp_5
        audio_data: validation_audio_logmels.hdf5
        text_data: validation_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence

    eval_data: # evaluation data
        dataset: ./data/exp_5
        audio_data: evaluation_audio_logmels.hdf5
        text_data: evaluation_text.csv
        text_embeds: sbert_embeds.pkl
        text_level: sentence

# Configure hyper-parameters
param_conf:
    num_epoch: 100
    batch_size: 64
    model: DualEncoderModel
    criterion: infonce_loss
    optimizer: AdamOptimizer
    lr_scheduler: ReduceLROnPlateau

# Model definitions
DualEncoderModel:
    name: DualEncoderModel
    out_norm: L2

    audio_enc:
        name: CNN14Encoder
        init: prior
        weight: ./pretrained_models/CNN14_300.pth
        trainable: true
        out_dim: 300

    text_enc:
        name: SentBERTBaseEncoder
        init: prior
        out_dim: 300

# Criteria
criteria:
    infonce_loss:
        name: LogSoftmaxLoss
        args:
            temperature: 0.07
            dist: dot_product # dot_product, cosine_similarity

# Optimizer definitions
AdamOptimizer:
    name: Adam
    args:
        lr: 0.001
        weight_decay: 0.0

# Learning rate scheduler definitions
ReduceLROnPlateau:
    name: ReduceLROnPlateau
    args:
        mode: min
        factor: 0.1
        patience: 5
        threshold: 0.005
        threshold_mode: abs
        min_lr: 0.000001
        verbose: true

# Weights & Biases Configuration
wandb_conf:
    project: "audio_dl_baseline"
